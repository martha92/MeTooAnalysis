{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: \n",
    "The objective is to build a sentiment classifier using a combination of a lexicon-based classifier, **TextBlob**, and a **HashTag sentiment classifier** and combine the sentiment score of the two to give an accurate sentiment classification to each tweet.\n",
    "\n",
    "* **TextBlob:** is a python library to provide the sentiment/polarity score to each input string/tweet. \n",
    "* **HashTag Sentiment Classifier:** Is based on the belief propagation approach mentioned in the paper.\n",
    "> __[Topic Sentiment Analysis in Twitter: A Graph-based Hashtag Sentiment Classification Approach](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.462.3827&rep=rep1&type=pdf)__\n",
    "\n",
    "The idea is to assign a sentiment value to each hashtag based on the sentiment value of the neighbouring/co-occurring hashtags. i.e. a hashtag used frequently with other hashtags with negative sentiment value will most likely to be having a negative sentiment value itself.\n",
    "\n",
    "The task of assigning sentiment label ['neg', 'neutral', 'pos'] to each hashtag is broken in two scripts.\n",
    "\n",
    "1. **input_HG:** extract the sentiment probability of the hashtag based on the sentiments of the tweets in which hashtag occurs and extract the co-occurrence ratio between a pair of hashtags.\n",
    "2. **LBP:** run the loopy belief propagation to assign a sentiment value to each hashtag based on its neighbouring/co-occurring hashtags.\n",
    "\n",
    "\n",
    "Finaly a sentiment classifier which is a combination of TextBlob output and HashTag sentiment classifier is used in *similarity_graph.get_hashtag_polarity()* to output sentiment for each tweet.\n",
    "\n",
    "**Note:** only most popular hashtags ~1000 are considered to keep running time realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools/ Technology Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Pickle:** for loading python objects from file\n",
    "* **Numpy:** For creating and performing matrix operations\n",
    "* **time:** Get the system time\n",
    "* **os:** To refer to operating system paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output \n",
    "The output for the script is a file containing the **id of hashtag, hastag, final label for hashtag, [positive + negative + neutral] probablity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID\tNode\tLabel\tpos\tneg\tneutral\r\n",
      "0\tthisisnotconsent\tneutral\t0.0\t0.0\t1.0\r\n",
      "1\tbhfyp\tpos\t0.9999999999999999\t0.0\t1.879366699436796e-41\r\n",
      "2\tigers\tneutral\t1.1487926839616725e-21\t0.0\t1.0\r\n",
      "3\tsuicide\tpos\t1.0\t0.0\t0.0\r\n"
     ]
    }
   ],
   "source": [
    "!cat ouput.results | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "from os.path import abspath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the output of *input_HG*.\n",
    "* **nodes:** List of most popular/frequently used hashtags.\n",
    "* **PI:** Sentiment probablity for each hashtag in the list based on the polarity value of the tweets in which the hashtag occured.\n",
    "* **SI:** Co-occurance ratio between a pair of hashtags. i.e. \n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\#(H_1,H_2)}{\\#(H_1) + \\#(H_2)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes = 1054\n"
     ]
    }
   ],
   "source": [
    "labels = ['pos', 'neg', 'neutral']\n",
    "pickle_in = open(\"hashtag.pickle\",\"rb\")\n",
    "[nodes, PI, SI] = pickle.load(pickle_in)\n",
    "num_nodes = len(nodes)\n",
    "print(\"Number of nodes = {}\".format(num_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timer to count the number of seconds for each loopy belief propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:    \n",
    "    \"\"\" Timer \"\"\"\n",
    "    def __enter__(self):\n",
    "        self.start = time.clock() # start\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.clock() # end\n",
    "        self.i = self.end - self.start # time taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Co-occurance ratio dictionary into a numpy matrix\n",
    "$ Shape(PSI) = \\#(Nodes) * \\#(Nodes) $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_psi(SI):\n",
    "    PSI = np.zeros((num_nodes, num_nodes))\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            try:\n",
    "                PSI[i][j] = SI[nodes[i], nodes[j]]\n",
    "                PSI[j][i] = SI[nodes[i], nodes[j]]\n",
    "            except:\n",
    "                try:\n",
    "                    PSI[i][j] = SI[nodes[j], nodes[i]]\n",
    "                    PSI[j][i] = SI[nodes[j], nodes[i]]\n",
    "                except:\n",
    "                    None\n",
    "    return PSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting sentiment probablity dictionary into a numpy matrix\n",
    "$ Shape(PHI) = \\#(Nodes) * \\#(Labels) $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phi(PI):\n",
    "    PHI = np.zeros((num_nodes, len(labels)))\n",
    "    for node in range(num_nodes):\n",
    "        for label in range(len(labels)):\n",
    "            try:\n",
    "                PHI[node][label] = PI[labels[label], nodes[node]]\n",
    "            except:\n",
    "                PHI[node][label] = 0.0\n",
    "    return PHI   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagation of belief from a node to its neighbours\n",
    "\n",
    "\n",
    "$ m_{i \\rightarrow j} (y_j ) \\leftarrow \\alpha \\sum_{y_i} \\psi_{i,j} (y_i, y_j ) \\phi_{i}(y_i) \\prod_{h_K \\in N(h_i) \\backslash h_j}  m_{k \\rightarrow i}(y_i)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMessage(i, PHIi, PSIi, label_j, labels, neighbours, messages):\n",
    "    m = { label: np.zeros(messages[label].shape[0]) for label in labels } # initialise\n",
    "    zeros = np.zeros(len(PSIi))\n",
    "\n",
    "    for neighbour in neighbours:\n",
    "        S = set(neighbours) - {neighbour}\n",
    "        for label in labels: # compute contribution to each neighbour from all other neighbours of i\n",
    "            m[label][neighbour] = np.prod([ messages[label][k,i] for k in S ]) + .00000001\n",
    "\n",
    "    return sum( np.multiply(PHIi[i] * (PSIi if label_i == label_j else zeros), m[label_i]) for i, label_i in enumerate(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the final score for each hashtag after belief propataion is done\n",
    "$ y_i \\leftarrow \\arg\\max_{y \\in \\{pos, neg\\}} \\alpha \\phi_i(y) \\prod_{h_j \\in N(h_i)}  m_{j \\rightarrow i}(y)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScores(i, PHIi, labels, neighbours, messages):\n",
    "    scores = { label: PHIi[label_i] * np.prod([ messages[label][j,i] for j in neighbours ]) for label_i, label in enumerate(labels) }\n",
    "    # normalise scores\n",
    "    alpha = sum( scores[label] for label in labels )\n",
    "    if alpha:\n",
    "        for label in labels:\n",
    "            scores[label] *= 1 / alpha\n",
    "\n",
    "    return scores\n",
    "\n",
    "def getLabels(scores):\n",
    "    return [ max(score, key=score.get) for score in scores ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running loopy belief propagation iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LBP(labels, nodes, PI, SI):\n",
    "    with Timer() as t: # do some extra initialisation\n",
    "        PSI = get_psi(SI) # Convert the SI value to matrix representation\n",
    "        PHI = get_phi(PI) # Convert sentiment probablity to matrix representation\n",
    "        messages = { label: (PSI > 0).astype(float) for label in labels } # Initialize the message values to 1 for co-occuring nodes \n",
    "        neighbours = [ list(np.nonzero(PSI[i])[0]) for i in range(num_nodes) ] # extract neighbours for all nodes\n",
    "        print('PSI = {}, PHI = {}'.format(PHI.shape, PSI.shape))\n",
    "    print(\"\\nStarting propagation on {} node hashtag graph\".format(num_nodes))\n",
    "\n",
    "    loops = 0 # loop variable t\n",
    "    while True:\n",
    "        with Timer() as t: # compute messages\n",
    "            loops += 1\n",
    "            old_messages = { label: messages[label].copy() for label in labels } # archive messages\n",
    "            for i in range(num_nodes): # for each node\n",
    "                for label in labels: # compute message from node i to its neighbours\n",
    "                    messages[label][i,:] = getMessage(i, PHI[i,:].flatten(), PSI[i,:].flatten(), label, labels, neighbours[i], old_messages)\n",
    "                    \n",
    "                alpha = [ alpha for alpha in map(lambda x : 1/x if x else 1, np.sum([ messages[label][i,:] for label in labels ], axis=0)) ] # compute normaliser\n",
    "                for label in labels:\n",
    "                    messages[label][i,:] *= alpha # normalise messages\n",
    "                \n",
    "        print(\"[{:.3f}s] Loop {} completed.\\tMessage change: {}\".format(t.i, loops, np.sum([ abs(old_messages[label] - messages[label]) for label in labels ])))\n",
    "\n",
    "        if np.product([ np.allclose(old_messages[label], messages[label]) for label in labels ]) or loops > 10: # halt if messages stop changing\n",
    "            break\n",
    "\n",
    "    print(\"Propagation complete.\\n\")\n",
    "    \n",
    "    with Timer() as t: # compute final labels\n",
    "        scores = [ getScores(i, PHI[i,:], labels, neighbours[i], messages) for i in range(num_nodes)]\n",
    "        results = getLabels(scores)\n",
    "\n",
    "    print(\"[{:.3f}s] Final labels computed. Objective value: {}\".format(t.i, sum( max(score.values()) for score in scores )))\n",
    "\n",
    "    return results, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResults(labels, nodes, results, scores):\n",
    "    filename = 'ouput.results'\n",
    "    with open(filename, 'w') as fo:\n",
    "        fo.write(\"ID\\tNode\\tLabel\\t{}\\n\".format('\\t'.join(labels)))\n",
    "        for i in range(len(nodes)):\n",
    "            fo.write(\"{}\\t{}\\t{}\\t{}\\n\".format(i, nodes[i], results[i], '\\t'.join(str(scores[i][label]) for label in labels)))\n",
    "            \n",
    "    print(\"Results written to '{}'.\".format(abspath(filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the algorithm and outputing the result to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSI = (1054, 3), PHI = (1054, 1054)\n",
      "\n",
      "Starting propagation on 1054 node hashtag graph\n",
      "[198.643s] Loop 1 completed.\tMessage change: 439288.0000000003\n",
      "[199.460s] Loop 2 completed.\tMessage change: 157.43698451538225\n",
      "[199.616s] Loop 3 completed.\tMessage change: 9.698930472669753\n",
      "[198.841s] Loop 4 completed.\tMessage change: 0.1042547165131901\n",
      "[201.808s] Loop 5 completed.\tMessage change: 0.0005052888974507859\n",
      "[200.982s] Loop 6 completed.\tMessage change: 2.3558844685393022e-05\n",
      "[200.157s] Loop 7 completed.\tMessage change: 5.701835024207925e-07\n",
      "Propagation complete.\n",
      "\n",
      "[0.248s] Final labels computed. Objective value: nan\n",
      "Results written to '/home/ipsita_proff/bd_project/ouput.results'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ipsita_proff/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in double_scalars\n",
      "  import sys\n",
      "/home/ipsita_proff/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "results, scores = LBP(labels, nodes, PI, SI) # run propagation\n",
    "printResults(labels, nodes, results, scores) # print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
