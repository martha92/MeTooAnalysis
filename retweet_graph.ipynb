{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.appName('retweet graph').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(num):\n",
    "    return num != num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retweetgraph(data, filename, num = 5):\n",
    "    retweets = []\n",
    "    for line in data:\n",
    "        retweets.append([line[0], line[1]])\n",
    "    # Find out nodes with more than 'num' weighted degree\n",
    "    nodes = dict()\n",
    "    for line in retweets:\n",
    "        if line[0] not in nodes:\n",
    "            nodes[line[0]] = 0\n",
    "        nodes[line[0]] += 1\n",
    "        if line[1] not in nodes:\n",
    "            nodes[line[1]] = 0\n",
    "        nodes[line[1]] += 1\n",
    "\n",
    "    for i in list(nodes):\n",
    "        if nodes[i] <= num:\n",
    "            del nodes[i]\n",
    "            \n",
    "    # Find undirected weighted edges without self loop\n",
    "    temp = dict()\n",
    "    for retweet in retweets:\n",
    "        if retweet[0] == retweet[1]:\n",
    "            continue\n",
    "        if retweet[0] in nodes and retweet[1] in nodes:\n",
    "            if (retweet[0], retweet[1]) in temp:\n",
    "                temp[(retweet[0], retweet[1])] += 1\n",
    "            elif (retweet[1], retweet[0]) in temp:\n",
    "                temp[(retweet[1], retweet[0])] += 1\n",
    "            else:\n",
    "                temp[(retweet[0], retweet[1])] = 1\n",
    "    edges = list(temp.items())\n",
    "    f = open(filename, 'w')\n",
    "    for edge in edges:\n",
    "        f.write('{}\\t{}\\t{}\\n'.format(edge[0][0], edge[0][1], edge[1]))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('humans1.csv', sep=';', index_col='id', usecols = ['id', 'screen_name', 'is_retweet', 'text_retweet'])\n",
    "df1 = df[df.apply(lambda x: (x['is_retweet']==True) and (isNaN(x['text_retweet'])==False), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "def get_retweet_user(text_retweet):\n",
    "    try:\n",
    "        return re.match(r\"RT @(\\S+):.*\", text_retweet).group(1)\n",
    "    except:\n",
    "        return 'False'\n",
    "    \n",
    "df1['retweet_username'] = df1.apply(lambda x: get_retweet_user(x['text_retweet']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweetgraph(df1[df1['retweet_username'] != 'False'][['screen_name', 'retweet_username']].values.tolist(), 'retweetgraph.txt', 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put graphframe jar file matching the pyspark version in current directory\n",
    "spark-sumbit --packages graphframe:graphframes:<graphframe version> retweet_graph_lpa.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hadoop/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hadoop/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/hadoop/anaconda3/lib/python3.7/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-420d91f3-49d7-4720-bf18-beece5282171;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.7.0-spark2.4-s_2.11 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 262ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.7.0-spark2.4-s_2.11 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-420d91f3-49d7-4720-bf18-beece5282171\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/13ms)\n",
      "2019-04-08 06:53:03 WARN  Utils:66 - Your hostname, vikash-HP-Notebook resolves to a loopback address: 127.0.1.1; using 172.17.0.1 instead (on interface docker0)\n",
      "2019-04-08 06:53:03 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "2019-04-08 06:53:04 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2019-04-08 06:53:06 INFO  SparkContext:54 - Running Spark version 2.4.0\n",
      "2019-04-08 06:53:06 INFO  SparkContext:54 - Submitted application: retweet graph lpa\n",
      "2019-04-08 06:53:06 INFO  SecurityManager:54 - Changing view acls to: hadoop\n",
      "2019-04-08 06:53:06 INFO  SecurityManager:54 - Changing modify acls to: hadoop\n",
      "2019-04-08 06:53:06 INFO  SecurityManager:54 - Changing view acls groups to: \n",
      "2019-04-08 06:53:06 INFO  SecurityManager:54 - Changing modify acls groups to: \n",
      "2019-04-08 06:53:06 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()\n",
      "2019-04-08 06:53:06 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 43917.\n",
      "2019-04-08 06:53:06 INFO  SparkEnv:54 - Registering MapOutputTracker\n",
      "2019-04-08 06:53:06 INFO  SparkEnv:54 - Registering BlockManagerMaster\n",
      "2019-04-08 06:53:06 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2019-04-08 06:53:06 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n",
      "2019-04-08 06:53:06 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-7801f9d2-cf34-4181-a589-77110d983293\n",
      "2019-04-08 06:53:06 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB\n",
      "2019-04-08 06:53:06 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n",
      "2019-04-08 06:53:06 INFO  log:192 - Logging initialized @3954ms\n",
      "2019-04-08 06:53:06 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "2019-04-08 06:53:06 INFO  Server:419 - Started @4057ms\n",
      "2019-04-08 06:53:06 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2019-04-08 06:53:06 INFO  AbstractConnector:278 - Started ServerConnector@5a02e5e4{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}\n",
      "2019-04-08 06:53:06 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4041.\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1b823b87{/jobs,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@d01b435{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@74efb64f{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@45d73245{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@457796d6{/stages,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4ca31df9{/stages/json,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7e3a040c{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2fa3357f{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1975ee3{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@19288da1{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3f2c3f33{/storage,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@992fd64{/storage/json,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@32c9caf3{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6fd11c8e{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@315a73e0{/environment,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2f3dc172{/environment/json,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@535f1834{/executors,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@340d4bbd{/executors/json,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@56aafbaf{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@17dd9659{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3ce8b22e{/static,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@26f28618{/,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@438b10eb{/api,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5119c99b{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62d5194d{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:06 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://172.17.0.1:4041\n",
      "2019-04-08 06:53:06 INFO  SparkContext:54 - Added JAR file:///home/hadoop/.ivy2/jars/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar at spark://172.17.0.1:43917/jars/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar with timestamp 1554720786970\n",
      "2019-04-08 06:53:06 INFO  SparkContext:54 - Added JAR file:///home/hadoop/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://172.17.0.1:43917/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1554720786972\n",
      "2019-04-08 06:53:07 INFO  SparkContext:54 - Added file file:///home/hadoop/.ivy2/jars/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar at file:///home/hadoop/.ivy2/jars/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar with timestamp 1554720787022\n",
      "2019-04-08 06:53:07 INFO  Utils:54 - Copying /home/hadoop/.ivy2/jars/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar to /tmp/spark-645e57e1-811b-459b-9f3b-080145d3954e/userFiles-21eca652-c8f6-4095-9945-5ff24d571332/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar\n",
      "2019-04-08 06:53:07 INFO  SparkContext:54 - Added file file:///home/hadoop/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///home/hadoop/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1554720787084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-08 06:53:07 INFO  Utils:54 - Copying /home/hadoop/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-645e57e1-811b-459b-9f3b-080145d3954e/userFiles-21eca652-c8f6-4095-9945-5ff24d571332/org.slf4j_slf4j-api-1.7.16.jar\n",
      "2019-04-08 06:53:07 INFO  Executor:54 - Starting executor ID driver on host localhost\n",
      "2019-04-08 06:53:07 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34125.\n",
      "2019-04-08 06:53:07 INFO  NettyBlockTransferService:54 - Server created on 172.17.0.1:34125\n",
      "2019-04-08 06:53:07 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2019-04-08 06:53:07 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 172.17.0.1, 34125, None)\n",
      "2019-04-08 06:53:07 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 172.17.0.1:34125 with 366.3 MB RAM, BlockManagerId(driver, 172.17.0.1, 34125, None)\n",
      "2019-04-08 06:53:07 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 172.17.0.1, 34125, None)\n",
      "2019-04-08 06:53:07 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 172.17.0.1, 34125, None)\n",
      "2019-04-08 06:53:07 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@750536e5{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:07 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hadoop/Documents/BD_Project/MeTooAnalysis-master/twitter/spark-warehouse').\n",
      "2019-04-08 06:53:07 INFO  SharedState:54 - Warehouse path is 'file:/home/hadoop/Documents/BD_Project/MeTooAnalysis-master/twitter/spark-warehouse'.\n",
      "2019-04-08 06:53:07 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4bc80e99{/SQL,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:07 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@34f22086{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:07 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7cb40a40{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:07 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@32d15357{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:07 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4479044f{/static/sql,null,AVAILABLE,@Spark}\n",
      "2019-04-08 06:53:08 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\n",
      "+-----+--------------+-----+\n",
      "|   id|          name|label|\n",
      "+-----+--------------+-----+\n",
      "|  964|   cheeryrosie|81292|\n",
      "| 2214|   sbuchbinder|81304|\n",
      "| 2529|    Crone4life|81292|\n",
      "| 3506|    sjferg1252|81269|\n",
      "| 9968|      tgweeded|14121|\n",
      "|14719|alexan_gongora|81292|\n",
      "|81293|JackieOniceass|  962|\n",
      "|84548|         NARAL|14039|\n",
      "|  418|       pkanthi|81315|\n",
      "|18598|         hirr4|14123|\n",
      "+-----+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+--------------+-------------------+\n",
      "|   id|          name|           pagerank|\n",
      "+-----+--------------+-------------------+\n",
      "|  964|   cheeryrosie| 0.3507565086074427|\n",
      "| 2214|   sbuchbinder| 0.3507565086074427|\n",
      "| 2529|    Crone4life| 0.3507565086074427|\n",
      "| 3506|    sjferg1252| 0.3507565086074427|\n",
      "| 9968|      tgweeded|0.39416262654761375|\n",
      "|14719|alexan_gongora| 0.3507565086074427|\n",
      "|81293|JackieOniceass| 2.4492937318730017|\n",
      "|84548|         NARAL| 1.6355775996365054|\n",
      "|  418|       pkanthi| 0.3507565086074427|\n",
      "|18598|         hirr4| 0.6980054521288109|\n",
      "+-----+--------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "2019-04-08 06:53:53 WARN  TaskSetManager:66 - Stage 348 contains a task of very large size (157 KB). The maximum recommended task size is 100 KB.\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --packages graphframes:graphframes:0.7.0-spark2.4-s_2.11 retweet_graph_lpa.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.parquet('retweet_lpa.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+---------------+------------------+\n",
      "|             id|        label|           name|          pagerank|\n",
      "+---------------+-------------+---------------+------------------+\n",
      "|   rissmontrone|  94489280526|   rissmontrone|1.9903495694739561|\n",
      "|    NaplesRocks| 506806140937|    NaplesRocks|0.5686619742746646|\n",
      "|   MarciaBelsky| 738734374914|   MarciaBelsky|0.5686619742746646|\n",
      "|  JaninaLarssen| 584115552258|  JaninaLarssen|0.6249595097278564|\n",
      "|    JustJen2015| 197568495618|    JustJen2015|0.8501496515406236|\n",
      "|       guardian| 429496729602|       guardian| 0.709405812907644|\n",
      "|        NYCer99| 584115552258|        NYCer99|0.5686619742746646|\n",
      "|      _ItsToni_|1614907703299|      _ItsToni_|0.5686619742746646|\n",
      "|      Klassic54| 584115552258|      Klassic54|0.5686619742746646|\n",
      "|     xande18000| 111669149701|     xande18000|0.5686619742746646|\n",
      "|    Vaibhav_AAP| 730144440320|    Vaibhav_AAP|  1.88227113484914|\n",
      "|     hugomartin|1194000908294|     hugomartin|1.1316373288065826|\n",
      "|  unmaithamilan|  34359738376|  unmaithamilan|0.5686619742746646|\n",
      "|       Diggi840| 395136991240|       Diggi840|0.5686619742746646|\n",
      "|       jillienp| 730144440325|       jillienp|1.1316373288065826|\n",
      "| JeremyRobards7| 446676598784| JeremyRobards7| 1.410310129299882|\n",
      "|ScarletMagdalen| 876173328395|ScarletMagdalen|0.5686619742746646|\n",
      "|   ItsMamaCarol| 231928233993|   ItsMamaCarol|0.7563204257853039|\n",
      "|  DRUDGE_REPORT|1511828488192|  DRUDGE_REPORT|0.8501496515406236|\n",
      "|     kylekirkup|  77309411335|     kylekirkup|0.8501496515406236|\n",
      "+---------------+-------------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
